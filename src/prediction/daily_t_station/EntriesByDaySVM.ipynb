{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entries by Day using Linear Regression/SVM #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Setup.\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locationid</th>\n",
       "      <th>service_day</th>\n",
       "      <th>entries</th>\n",
       "      <th>name</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>service_datetime</th>\n",
       "      <th>fog</th>\n",
       "      <th>...</th>\n",
       "      <th>entries_weeks_ago_1</th>\n",
       "      <th>entries_weeks_ago_2</th>\n",
       "      <th>entries_weeks_ago_3</th>\n",
       "      <th>rain_predict</th>\n",
       "      <th>rain_fall_predict</th>\n",
       "      <th>snow_predict</th>\n",
       "      <th>snow_fall_predict</th>\n",
       "      <th>snow_accum</th>\n",
       "      <th>snow_accum_predict</th>\n",
       "      <th>dist_to_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-01 00:00:00</td>\n",
       "      <td> 1892</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-01 03:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-02 00:00:00</td>\n",
       "      <td> 5134</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-02 04:45:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 5733</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-03 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-04 00:00:00</td>\n",
       "      <td> 6125</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-04 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-05 00:00:00</td>\n",
       "      <td> 3410</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-05 04:15:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   locationid          service_day  entries           name line_1 line_2  \\\n",
       "0        1002  2013-01-01 00:00:00     1892  Andrew Square    Red    NaN   \n",
       "1        1002  2013-01-02 00:00:00     5134  Andrew Square    Red    NaN   \n",
       "2        1002  2013-01-03 00:00:00     5733  Andrew Square    Red    NaN   \n",
       "3        1002  2013-01-04 00:00:00     6125  Andrew Square    Red    NaN   \n",
       "4        1002  2013-01-05 00:00:00     3410  Andrew Square    Red    NaN   \n",
       "\n",
       "        lat       lon     service_datetime  fog ...   entries_weeks_ago_1  \\\n",
       "0  42.32955 -71.05696  2013-01-01 03:00:00    0 ...                   NaN   \n",
       "1  42.32955 -71.05696  2013-01-02 04:45:00    0 ...                   NaN   \n",
       "2  42.32955 -71.05696  2013-01-03 05:00:00    0 ...                   NaN   \n",
       "3  42.32955 -71.05696  2013-01-04 05:00:00    0 ...                   NaN   \n",
       "4  42.32955 -71.05696  2013-01-05 04:15:00    0 ...                   NaN   \n",
       "\n",
       "   entries_weeks_ago_2  entries_weeks_ago_3  rain_predict  rain_fall_predict  \\\n",
       "0                  NaN                  NaN             0                  0   \n",
       "1                  NaN                  NaN             0                  0   \n",
       "2                  NaN                  NaN             0                  0   \n",
       "3                  NaN                  NaN             0                  0   \n",
       "4                  NaN                  NaN             0                  0   \n",
       "\n",
       "   snow_predict  snow_fall_predict  snow_accum  snow_accum_predict  \\\n",
       "0             0                  0           0                   0   \n",
       "1             0                  0           0                   0   \n",
       "2             0                  0           0                   0   \n",
       "3             0                  0           0                   0   \n",
       "4             1                  0           0                   0   \n",
       "\n",
       "   dist_to_center  \n",
       "0        3.404767  \n",
       "1        3.404767  \n",
       "2        3.404767  \n",
       "3        3.404767  \n",
       "4        3.404767  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../../../data/mbta_daily.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 47901\n",
      "Remaining: 34294\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locationid</th>\n",
       "      <th>service_day</th>\n",
       "      <th>entries</th>\n",
       "      <th>name</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>service_datetime</th>\n",
       "      <th>fog</th>\n",
       "      <th>...</th>\n",
       "      <th>entries_weeks_ago_1</th>\n",
       "      <th>entries_weeks_ago_2</th>\n",
       "      <th>entries_weeks_ago_3</th>\n",
       "      <th>rain_predict</th>\n",
       "      <th>rain_fall_predict</th>\n",
       "      <th>snow_predict</th>\n",
       "      <th>snow_fall_predict</th>\n",
       "      <th>snow_accum</th>\n",
       "      <th>snow_accum_predict</th>\n",
       "      <th>dist_to_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-01 00:00:00</td>\n",
       "      <td> 1892</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-01 03:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-02 00:00:00</td>\n",
       "      <td> 5134</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-02 04:45:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 5733</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-03 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-04 00:00:00</td>\n",
       "      <td> 6125</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-04 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-07 00:00:00</td>\n",
       "      <td> 5998</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.32955</td>\n",
       "      <td>-71.05696</td>\n",
       "      <td> 2013-01-07 04:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.404767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   locationid          service_day  entries           name line_1 line_2  \\\n",
       "0        1002  2013-01-01 00:00:00     1892  Andrew Square    Red    NaN   \n",
       "1        1002  2013-01-02 00:00:00     5134  Andrew Square    Red    NaN   \n",
       "2        1002  2013-01-03 00:00:00     5733  Andrew Square    Red    NaN   \n",
       "3        1002  2013-01-04 00:00:00     6125  Andrew Square    Red    NaN   \n",
       "6        1002  2013-01-07 00:00:00     5998  Andrew Square    Red    NaN   \n",
       "\n",
       "        lat       lon     service_datetime  fog ...   entries_weeks_ago_1  \\\n",
       "0  42.32955 -71.05696  2013-01-01 03:00:00    0 ...                   NaN   \n",
       "1  42.32955 -71.05696  2013-01-02 04:45:00    0 ...                   NaN   \n",
       "2  42.32955 -71.05696  2013-01-03 05:00:00    0 ...                   NaN   \n",
       "3  42.32955 -71.05696  2013-01-04 05:00:00    0 ...                   NaN   \n",
       "6  42.32955 -71.05696  2013-01-07 04:00:00    0 ...                   NaN   \n",
       "\n",
       "   entries_weeks_ago_2  entries_weeks_ago_3  rain_predict  rain_fall_predict  \\\n",
       "0                  NaN                  NaN             0                  0   \n",
       "1                  NaN                  NaN             0                  0   \n",
       "2                  NaN                  NaN             0                  0   \n",
       "3                  NaN                  NaN             0                  0   \n",
       "6                  NaN                  NaN             0                  0   \n",
       "\n",
       "   snow_predict  snow_fall_predict  snow_accum  snow_accum_predict  \\\n",
       "0             0                  0           0                   0   \n",
       "1             0                  0           0                   0   \n",
       "2             0                  0           0                   0   \n",
       "3             0                  0           0                   0   \n",
       "6             0                  0           0                   0   \n",
       "\n",
       "   dist_to_center  \n",
       "0        3.404767  \n",
       "1        3.404767  \n",
       "2        3.404767  \n",
       "3        3.404767  \n",
       "6        3.404767  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Original: \" + str(len(data)))\n",
    "\n",
    "data = data[(pd.DatetimeIndex(data['service_day']).weekday != 5) & (pd.DatetimeIndex(data['service_day']).weekday != 6)]\n",
    "\n",
    "print(\"Remaining: \" + str(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "def predict(station, x_cols, predictor, parameters, ape_threshold, state, drop_outliers = False):\n",
    "    # Copy the station so we don't manipulate the original\n",
    "    station = station.copy()\n",
    "    \n",
    "    # Get the columns of the dataframe.\n",
    "    cols = list(station.columns)\n",
    "    \n",
    "    # Determine the indices of the columns.\n",
    "    y_col_indices = [0] + list(np.where([col == 'entries' for col in cols])[0] + 1)\n",
    "    x_col_indices = [0] + list(np.where([col in x_cols for col in cols])[0] + 1)\n",
    "    \n",
    "    # Make sure none of the predictor fields are null.\n",
    "    for col in x_cols:\n",
    "        station = station[pd.notnull(station[col])]\n",
    "    \n",
    "    # Remove any entries where no one was there...\n",
    "    station = station[station['entries'] > 50]\n",
    "    \n",
    "    # Reset the station indices, we have to reset twice so the matrix values gets the index column.\n",
    "    station = station.reset_index()\n",
    "    station.drop('index', axis=1, inplace=True)\n",
    "    station = station.reset_index()\n",
    "    \n",
    "    # Get the dataframe as a matrix where the first column is the index.\n",
    "    matrix = station.values\n",
    "    \n",
    "    # Slice so the y only contains 2 column (index, entries)\n",
    "    #  and the x is a matrix that contains the index and all the predictors.\n",
    "    y = matrix[:,y_col_indices]\n",
    "    x = matrix[:,x_col_indices]\n",
    "    \n",
    "    # Split the data set into a train and test.\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.2, random_state=state)\n",
    "    \n",
    "    # Convert the train and test sets into a format sklean fit() expects.\n",
    "    x_train_fit = np.matrix(x_train[:,1:], dtype=np.float32)\n",
    "    y_train_fit = np.array([v[0] for v in y_train[:,1:]], dtype=np.uint16)\n",
    "    \n",
    "    x_test_fit = np.matrix(x_test[:,1:], dtype=np.float32)\n",
    "    y_test_fit = np.array([v[0] for v in y_test[:,1:]], dtype=np.uint16)\n",
    "    \n",
    "    # Train using a grid search based on the parameters provided.\n",
    "    clf = grid_search.GridSearchCV(predictor, parameters, scoring='mean_squared_error', cv=10)\n",
    "    clf.fit(x_train_fit, y_train_fit)\n",
    "    \n",
    "    # Determine what the best model was.\n",
    "    model = clf.best_estimator_\n",
    "    \n",
    "    # Predict using the test set.\n",
    "    y_pred_fit = model.predict(x_test_fit)\n",
    "    \n",
    "    # Determine the absolute percent errors.\n",
    "    apes = np.abs(y_pred_fit - y_test_fit) / y_test_fit\n",
    "    \n",
    "    # Determine any \"extreme\" outliers by determining large percent errors.\n",
    "    ape_indices = apes >= ape_threshold\n",
    "    outlier_indices = y_test[ape_indices][:,0]\n",
    "    outliers = station.iloc[outlier_indices]\n",
    "    outliers['entries_predicted'] = y_pred_fit[ape_indices]\n",
    "    outliers['entries_ape'] = apes[ape_indices]\n",
    "    \n",
    "    # Remove any percent difference that would cause an error in calculating the mean.\n",
    "    apes = apes[apes < float('+inf')]\n",
    "    if (drop_outliers):\n",
    "        apes = apes[apes < ape_threshold]\n",
    "    \n",
    "    return model, apes, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_outliers(station, x_cols, model):\n",
    "    outliers = pd.DataFrame()\n",
    "\n",
    "    for index, row in station.iterrows():\n",
    "        y = row['entries']\n",
    "        x = row.as_matrix(x_cols)\n",
    "        \n",
    "        nan = False\n",
    "        for i in x:\n",
    "            if np.isnan(i):\n",
    "                nan= True\n",
    "                break\n",
    "        if nan:\n",
    "            continue\n",
    "        \n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        ape = np.abs(y_pred[0] - y) / y\n",
    "\n",
    "        if (ape > .2):\n",
    "            row['entries_predict'] = y_pred[0]\n",
    "            row['entries_ape'] = ape\n",
    "\n",
    "            outliers = outliers.append(row)\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_test(stationids, x_cols):\n",
    "    accuracies = []\n",
    "    outliers_count = []\n",
    "    \n",
    "    for stationid in stationids:\n",
    "        station = data[data['locationid'] == stationid]\n",
    "\n",
    "        #predictor = svm.SVR()\n",
    "        #parameters = {'kernel': ('linear', 'rbf'), 'C': range(1, 2)}\n",
    "        predictor = linear_model.LinearRegression()\n",
    "        parameters = {}\n",
    "        \n",
    "        # Train and predict X times, each with a different train/test set using a random state.\n",
    "        mean_apes = []\n",
    "        mean_outliers = []\n",
    "        for state in xrange(10):\n",
    "            model, apes, outliers = predict(station, x_cols, predictor, parameters, .2, state, True)\n",
    "            #outliers.to_csv(\"outliers-tmp-\" + str(stationid) + \".csv\", index=False)\n",
    "            #print(np.mean(apes))\n",
    "            mean_apes.append(np.mean(apes))\n",
    "            mean_outliers.append(len(outliers))\n",
    "\n",
    "        #outliers = find_outliers(station, x_cols, model)\n",
    "        #outliers.to_csv(\"outliers-\" + str(stationid) + \".csv\", index=False)\n",
    "        \n",
    "        accuracies.append(1 - np.mean(mean_apes))\n",
    "        outliers_count.append(np.mean(mean_outliers))\n",
    "        \n",
    "        print(station.iloc[0][\"name\"] + \": \" + str(accuracies[-1]))# + \" (\" + str(outliers_count[-1]) + \")\"\n",
    "    \n",
    "    return accuracies, outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationids = [\n",
    "    1020, # Braintree\n",
    "    1032, # Alewife\n",
    "    1035, # Harvard\n",
    "    1039, # Downtown\n",
    "    1043, # Ashmont\n",
    "    1059, # Kenmore\n",
    "    1075, # North\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braintree: 0.940939670925\n",
      "Alewife: 0.945968636398\n",
      "Harvard: 0.945009409866\n",
      "Downtown Crossing: 0.945641917908\n",
      "Ashmont: 0.934765053715\n",
      "Kenmore Square: 0.923819531105\n",
      "North Station: 0.920146811654\n",
      "\n",
      "Mean Acc.: 0.93661300451\n",
      "Var Ac.: 9.94988830585e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies, outliers_count = features_test(stationids, ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'entries_weeks_ago_1'])\n",
    "print(\"\")\n",
    "print(\"Mean Acc.: \" + str(np.mean(accuracies)))\n",
    "print(\"Var Ac.: \" + str(np.var(accuracies)))\n",
    "print(\"\")\n",
    "#print(\"Mean Outliers.: \" + str(np.mean(outliers_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braintree: 0.824133935605 (10.9)\n",
      "Alewife: 0.802462865258 (10.3)\n",
      "Harvard: 0.851474464359 (10.5)\n",
      "Downtown Crossing: 0.619149123287 (9.1)\n",
      "Ashmont: 0.846935852425 (8.5)\n",
      "North Station: 0.716187723508 (16.1)\n",
      "\n",
      "Mean Acc.: 0.776723994074\n",
      "Var Ac.: 0.00698700164406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies, outliers_count = features_test(stationids, ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'entries_weeks_ago_1', 'snow_fall'])\n",
    "print(\"\")\n",
    "print(\"Mean Acc.: \" + str(np.mean(accuracies)))\n",
    "print(\"Var Ac.: \" + str(np.var(accuracies)))\n",
    "print(\"\")\n",
    "#print(\"Mean Outliers.: \" + str(np.mean(outliers_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braintree: 0.813675263564 (11.2)\n",
      "Alewife: 0.792026420638 (11.3)\n",
      "Harvard: 0.841790863532 (11.1)\n",
      "Downtown Crossing: 0.447795204618 (9.1)\n",
      "Ashmont: 0.829769979854 (10.0)\n",
      "North Station: 0.711130771263 (16.6)\n",
      "\n",
      "Mean Acc.: 0.739364750578\n",
      "Var Ac.: 0.0187949126081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies, outliers_count = features_test(stationids, ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'entries_weeks_ago_1', 'snow'])\n",
    "print(\"\")\n",
    "print(\"Mean Acc.: \" + str(np.mean(accuracies)))\n",
    "print(\"Var Ac.: \" + str(np.var(accuracies)))\n",
    "print(\"\")\n",
    "#print(\"Mean Outliers.: \" + str(np.mean(outliers_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braintree: 0.82811880225 (10.8)\n",
      "Alewife: 0.802076521309 (10.1)\n",
      "Harvard: 0.849448148284 (10.6)\n",
      "Downtown Crossing: 0.59284369088 (9.7)\n",
      "Ashmont: 0.843927997629 (9.1)\n",
      "North Station: 0.712857503969 (16.6)\n",
      "\n",
      "Mean Acc.: 0.771545444053\n",
      "Var Ac.: 0.00846989130386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies, outliers_count = features_test(stationids, ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'entries_weeks_ago_1', 'snow_accum'])\n",
    "print(\"\")\n",
    "print(\"Mean Acc.: \" + str(np.mean(accuracies)))\n",
    "print(\"Var Ac.: \" + str(np.var(accuracies)))\n",
    "print(\"\")\n",
    "#print(\"Mean Outliers.: \" + str(np.mean(outliers_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braintree: 0.82795100997 (10.6)\n",
      "Alewife: 0.802781192496 (10.0)\n",
      "Harvard: 0.851274557388 (10.5)\n",
      "Downtown Crossing: 0.629702943666 (9.2)\n",
      "Ashmont: 0.845384386198 (8.8)\n",
      "North Station: 0.715182116487 (16.5)\n",
      "\n",
      "Mean Acc.: 0.778712701034\n",
      "Var Ac.: 0.00649234715203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies, outliers_count = features_test(stationids, ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'entries_weeks_ago_1', 'snow_fall', 'snow_accum'])\n",
    "print(\"\")\n",
    "print(\"Mean Acc.: \" + str(np.mean(accuracies)))\n",
    "print(\"Var Ac.: \" + str(np.var(accuracies)))\n",
    "print(\"\")\n",
    "#print(\"Mean Outliers.: \" + str(np.mean(outliers_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
