{
 "metadata": {
  "name": "",
  "signature": "sha256:4e3d0d22b454f8ace30b13d2729383595b209ac207b16e848e351e2fc8b588b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "AM-PM Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This script explores predictive models for p.m. T ridership based on known information."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setup & Configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import matplotlib, matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import sklearn, sklearn.linear_model, sklearn.ensemble\n",
      "\n",
      "# Display settings.\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Paths.\n",
      "path_gatecount = '../../../../data/gatecount_aggregate.csv'\n",
      "path_weather = '../../../../data/weather/weather_clean.csv'\n",
      "path_weather_normals = '../../../data/weather/boston_daily_normals.csv'\n",
      "path_locations = '../../../../data/Stations.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to carefully define our afternoon prediction interval. Simply predicting ridership after 12:00 p.m. is arbitrary and may not be useful. Basing predictions around rush hour traffic are more likely to yield useful results, and might benefit from taking into account more information after noon."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Interval.\n",
      "feature_interval_start = 5\n",
      "feature_interval_end = 13\n",
      "interval_start = 15\n",
      "interval_end   = 20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section reads ridership and weather data. Ridership data is aggregated by service date. Models will therefore be accurate in that sense (vs. calendar date)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read entries. We want all columns\n",
      "gatecount = pd.read_csv(path_gatecount)\n",
      "# Ensure service day is a datetime.\n",
      "gatecount.service_day = pd.to_datetime(gatecount.service_day)\n",
      "gatecount.service_datetime = pd.to_datetime(gatecount.service_datetime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatecount.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read weather.\n",
      "weather = pd.read_csv(path_weather)\n",
      "# Convert date column to datetime.\n",
      "weather.date = pd.to_datetime(weather.date)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weather.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read weather normals.\n",
      "weather_normals = pd.read_csv(path_weather_normals)\n",
      "weather_normals = weather_normals[['MoDy','MinT','AveT','MaxT','SnwG']]\n",
      "weather_normals.columns = ['month_day','temp_min_norm','temp_mean_norm','temp_max_norm','snow_on_ground_norm']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weather_normals.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read stations.\n",
      "locations = pd.read_csv(path_locations)\n",
      "# Reduce to columns that we know we need.\n",
      "locations = locations[['stationid','name']]\n",
      "locations.columns = ['locationid','station_name']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "locations.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Engineering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Weather Daily Normals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to eventually merge normals, we need to create synthetic dates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weather_normals_by_year = None\n",
      "for y in xrange(2014,2015+1):\n",
      "    weather_normals_y = weather_normals.copy()\n",
      "    if y % 4 != 0: # Remove leap year if needed.\n",
      "        weather_normals_y = weather_normals_y.loc[weather_normals_y.month_day != '2/29',:]\n",
      "    weather_normals_y.month_day = pd.to_datetime(pd.date_range('1/1/%d'%y, periods=(365 if y % 4 != 0 else 366), freq='D'))\n",
      "    if weather_normals_by_year is None:\n",
      "        weather_normals_by_year = weather_normals_y\n",
      "    else:\n",
      "        weather_normals_by_year = pd.concat([weather_normals_by_year, weather_normals_y])\n",
      "weather_normals_by_year.rename(columns={'month_day':'date'}, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weather_normals_by_year.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Aggregate at Hourly Level"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hourly resolution is a useful grain for analysis. Aggregate gate count data to hourly figures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate hour field.\n",
      "gatecount['hour'] = gatecount.service_datetime.map(lambda x: int(x.strftime('%H')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Group by hour.\n",
      "gatecount_hour = gatecount.groupby(['locationid','service_day','hour']).agg({'entries':np.sum, 'exits':np.sum}).reset_index(drop=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatecount_hour.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "AM & PM Ridership by Location"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following takes arbitrary inputs and returns a dataset filtered and integrated by time range."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def aggregate_by_time(start,end,col_prefix):\n",
      "    return (\n",
      "        gatecount_hour.ix[\n",
      "            (gatecount_hour.hour >= start) & (gatecount_hour.hour <= end), # Filter by pm interval\n",
      "            :\n",
      "        ]\n",
      "        .groupby(['locationid','service_day'])\n",
      "        .agg({'entries': np.sum, 'exits': np.sum})\n",
      "        .fillna(0)\n",
      "        .reset_index(drop=False)\n",
      "        .rename(columns={'entries':col_prefix+'_entries', 'exits':col_prefix+'_exits'})\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate for AM and PM.\n",
      "gatecount_am = aggregate_by_time(feature_interval_start, feature_interval_end, 'am')\n",
      "gatecount_pm = aggregate_by_time(interval_start, interval_end, 'pm')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatecount_am.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatecount_pm.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "AM Ridership for All Locations by Day"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the above information to aggregate entries/exits by date with location on columns. Very useful for feature generation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use .pivot() to easily transpose location id onto columns.\n",
      "\n",
      "# Generate entries.\n",
      "am_entries_by_location = gatecount_am.pivot(index='service_day', columns='locationid', values='am_entries')\n",
      "am_entries_by_location.columns = ['am_entries_loc'+str(c) for c in am_entries_by_location.columns]\n",
      "am_entries_by_location.fillna(0, inplace=True)\n",
      "am_entries_by_location.reset_index(drop=False, inplace=True)\n",
      "\n",
      "# Generate exits.\n",
      "am_exits_by_location = gatecount_am.pivot(index='service_day', columns='locationid', values='am_exits')\n",
      "am_exits_by_location.columns = ['am_exits_loc'+str(c) for c in am_exits_by_location.columns]\n",
      "am_exits_by_location.fillna(0, inplace=True)\n",
      "am_exits_by_location.reset_index(drop=False, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "am_entries_by_location.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "am_exits_by_location.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Hourly AM Ridership for All Locations by Day"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is where it gets complicated. We want a new dataset with the following:\n",
      "\n",
      "- Each row is a specific date and location.\n",
      "- Each column is a count of hourly entries.\n",
      "\n",
      "We want this in order to get a sense of *how* people are arriving at particular stations during the course of the day."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate data by location, hour.\n",
      "am_exits_by_location_hour = (\n",
      "    gatecount_hour\n",
      "    .ix[(gatecount_hour.hour >= feature_interval_start) & (gatecount_hour.hour <= feature_interval_end),:]\n",
      "    .pivot_table(index=['service_day','locationid'], columns='hour', values='exits')\n",
      ")\n",
      "# Fill NAs with 0.\n",
      "am_exits_by_location_hour.fillna(0, inplace=True)\n",
      "# Fix column names and reset index.\n",
      "am_exits_by_location_hour.columns = ['am_exits_'+str(c) for c in am_exits_by_location_hour.columns]\n",
      "am_exits_by_location_hour.reset_index(drop=False, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "am_exits_by_location_hour.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Historical PM Entries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Historical afternoon entries are a logical choice for predicting today's afternoon entries. This will build that dataset. It will also automatically restrict us to days that *have* that historical information (i.e., not dates at the start of our time range).\n",
      "\n",
      "Note that it isn't clear exactly how best to aggregate this yet. The significance of entries 7 days prior is obvious, but anything less is more complex. For example, on Wednesday the previous day is likely to be predictive in a different way than it would be for Monday.\n",
      "\n",
      "For now we'll end up generating two columns: 7 days prior and the sum of all days together."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DF container for historical entries.\n",
      "historical_pm_entries = gatecount_pm.ix[:,:2]\n",
      "# Iterate over days.\n",
      "for day in xrange(1,8):\n",
      "    # Copy existing data to get dailies.\n",
      "    day_entries = gatecount_pm[['locationid','service_day','pm_entries']].copy()\n",
      "    day_entries.service_day = day_entries.service_day + np.timedelta64(day,'D')\n",
      "    day_entries.rename(columns={'pm_entries':'pm_entries_'+str(day)+'d_prior'}, inplace=True)\n",
      "    historical_pm_entries = historical_pm_entries.merge(day_entries, on=['locationid','service_day'])\n",
      "    historical_pm_entries.fillna(0, inplace=True)\n",
      "\n",
      "# Separate for 7 days.\n",
      "historical_pm_entries_7d = historical_pm_entries.ix[:,['locationid','service_day','pm_entries_7d_prior']]\n",
      "\n",
      "# Sum for 6 prior days.\n",
      "historical_pm_entries_d1_d6 = (\n",
      "    historical_pm_entries[[c for c in historical_pm_entries.columns if c != 'pm_entries_7d_prior']]\n",
      "    .set_index(['locationid','service_day'])\n",
      "    .sum(axis=1)\n",
      "    .reset_index(drop=False)\n",
      ")\n",
      "historical_pm_entries_d1_d6.rename(columns={0:'pm_entries_d1_d6_prior'}, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "historical_pm_entries_7d.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "historical_pm_entries_d1_d6.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Merge"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time to merge all of this together into one big dataset. Yahoo!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Our starting point is PM ridership.\n",
      "prediction_dataset = gatecount_pm.drop('pm_exits', axis=1)\n",
      "\n",
      "# Merge AM exit aggregates.\n",
      "prediction_dataset = prediction_dataset.merge(gatecount_am, on=['locationid','service_day'])\n",
      "\n",
      "# Merge AM hourly exits.\n",
      "prediction_dataset = prediction_dataset.merge(am_exits_by_location_hour, on=['locationid','service_day'])\n",
      "\n",
      "# Merge all station AM entries and exits..\n",
      "prediction_dataset = prediction_dataset.merge(am_entries_by_location, on=['service_day'])\n",
      "prediction_dataset = prediction_dataset.merge(am_exits_by_location, on=['service_day'])\n",
      "\n",
      "# Merge historical PM entries.\n",
      "prediction_dataset = prediction_dataset.merge(historical_pm_entries_7d, on=['locationid','service_day'])\n",
      "prediction_dataset = prediction_dataset.merge(historical_pm_entries_d1_d6, on=['locationid','service_day'])\n",
      "\n",
      "# Merge weather.\n",
      "prediction_dataset = prediction_dataset.merge(weather, left_on='service_day', right_on='date')\n",
      "prediction_dataset = prediction_dataset.merge(weather_normals_by_year, left_on='service_day', right_on='date')\n",
      "prediction_dataset.drop(['date_x','date_y'], axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction_dataset.head(n=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prediction Setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These functions will generate a predictive model for a given location. They do several things:\n",
      "\n",
      "- Generate the appropriate subset of the data.\n",
      "- Randomly select a sample for testing.\n",
      "- Remove and columns with Null or NaN values.\n",
      "\n",
      "Specific functions may be more careful about regularization, etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Return valid prediction data based on specific constraints.\n",
      "\"\"\"\n",
      "def prediction_data(locationid, col_drop_threshold=0.1):\n",
      "    # Subset the dataframe.\n",
      "    loc_prediction_data = prediction_dataset.ix[prediction_dataset.locationid == locationid,:].copy()\n",
      "    \n",
      "    # Prefer to drop columns before rows.\n",
      "    # Drop columns that show too many nulls. Cleaning should have taken care of these.\n",
      "    problematic_cols = np.nonzero(loc_prediction_data.isnull().sum(axis=0)/len(loc_prediction_data) > col_drop_threshold)[0]\n",
      "    loc_prediction_data.drop(loc_prediction_data.columns[problematic_cols], axis=1, inplace=True)\n",
      "    # Drop any rows that have any nulls.\n",
      "    loc_prediction_data = loc_prediction_data.ix[(loc_prediction_data.isnull().sum(axis=1) == 0),:]\n",
      "    \n",
      "    # Drop columns we don't need.\n",
      "    non_predictive_cols = ['locationid','service_day']\n",
      "    loc_prediction_data.drop(non_predictive_cols, axis=1, inplace=True)\n",
      "    \n",
      "    # Return.\n",
      "    return loc_prediction_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Generate training and testing samples for a given location.\n",
      "\"\"\"\n",
      "def prediction_data_split(locationid, train_size):\n",
      "    # Generate data.\n",
      "    loc_prediction_data = prediction_data(locationid, col_drop_threshold)\n",
      "    \n",
      "    # Set up feature, output containers.\n",
      "    y = np.array(loc_prediction_data.pm_entries)\n",
      "    X = np.array(loc_prediction_data.drop('pm_entries',axis=1))\n",
      "    \n",
      "    # Generate samples and divide into training and testing sets.\n",
      "    train_indices = np.random.choice(range(len(X)), np.floor(len(X) * train_size), replace=False)\n",
      "    test_indices  = [i for i in range(len(X)) if i not in train_indices]\n",
      "    X_train = X[train_indices,:]\n",
      "    y_train = y[train_indices]\n",
      "    X_test  = X[test_indices,:]\n",
      "    y_test  = y[test_indices]\n",
      "    \n",
      "    # Return all data.\n",
      "    return X, y, X_train, y_train, X_test, y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Run a linear regression prediction for a given location.\n",
      "\"\"\"\n",
      "def lr_prediction(locationid, train_size = 0.8, col_drop_threshold=0.1, print_metrics = False):\n",
      "    # Get training and testing sets.\n",
      "    X, y, X_train, y_train, X_test, y_test = prediction_data_split(locationid, train_size)\n",
      "    \n",
      "    # Start a model and train it on the training data.\n",
      "    # This is useful for predictive purposes.\n",
      "    lr_model_train = sklearn.linear_model.LinearRegression()\n",
      "    lr_model_train.fit(X_train,y_train)\n",
      "    \n",
      "    # Generate predictions on test data.\n",
      "    y_hat_test = lr_model_train.predict(X_test)\n",
      "    \n",
      "    # Calculate performance metrics.\n",
      "    performance_test_r2 = sklearn.metrics.r2_score(y_test, y_hat_test)\n",
      "    performance_test_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test, y_hat_test))\n",
      "    performance_test_sample_mean = y_test.mean()\n",
      "    \n",
      "    # Output performance metrics.\n",
      "    if print_metrics:\n",
      "        print 'Model Built on Training Data vs. Performance on Test Data:'\n",
      "        print '\\tR^2 on test data:  %.4f' % performance_test_r2\n",
      "        print '\\tRMSE on test data: %.4f' % performance_test_rmse\n",
      "        print '\\tTest sample mean (for reference): %.4f' % performance_test_sample_mean\n",
      "    \n",
      "    # Start a model and train it on all data.\n",
      "    # This is useful for investigative purposes.\n",
      "    lr_model_all = sklearn.linear_model.LinearRegression()\n",
      "    lr_model_all.fit(X,y)\n",
      "    \n",
      "    # Generate predictions on all data.\n",
      "    y_hat_all = lr_model_all.predict(X)\n",
      "    \n",
      "    # Calculate performance metrics.\n",
      "    performance_all_r2 = sklearn.metrics.r2_score(y, y_hat_all)\n",
      "    performance_all_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y, y_hat_all))\n",
      "    performance_all_sample_mean = y.mean()\n",
      "    \n",
      "    # Output performance metrics.\n",
      "    if print_metrics:\n",
      "        print 'Model Built on All Data vs. Performance on All Data:'\n",
      "        print '\\tR^2 on all data:  %.4f' % performance_all_r2\n",
      "        print '\\tRMSE on all data: %.4f' % performance_all_rmse\n",
      "        print '\\tAll data sample mean (for reference): %.4f' % performance_all_sample_mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Run a random forest regression prediction for a given location.\n",
      "\"\"\"\n",
      "def rf_prediction(locationid, train_size = 0.8, col_drop_threshold=0.1, print_metrics = False):\n",
      "    # Get training and testing sets.\n",
      "    X, y, X_train, y_train, X_test, y_test = prediction_data_split(locationid, train_size)\n",
      "    \n",
      "    # Start a model and train it on the training data.\n",
      "    # This is useful for predictive purposes.\n",
      "    rf_model_train = sklearn.ensemble.RandomForestRegressor()\n",
      "    rf_model_train.fit(X_train,y_train)\n",
      "    \n",
      "    # Generate predictions on test data.\n",
      "    y_hat_test = rf_model_train.predict(X_test)\n",
      "    \n",
      "    # Calculate performance metrics.\n",
      "    performance_test_r2 = sklearn.metrics.r2_score(y_test, y_hat_test)\n",
      "    performance_test_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test, y_hat_test))\n",
      "    performance_test_sample_mean = y_test.mean()\n",
      "    \n",
      "    # Output performance metrics.\n",
      "    if print_metrics:\n",
      "        print 'Model Built on Training Data vs. Performance on Test Data:'\n",
      "        print '\\tR^2 on test data:  %.4f' % performance_test_r2\n",
      "        print '\\tRMSE on test data: %.4f' % performance_test_rmse\n",
      "        print '\\tTest sample mean (for reference): %.4f' % performance_test_sample_mean\n",
      "    \n",
      "    # Start a model and train it on all data.\n",
      "    # This is useful for investigative purposes.\n",
      "    rf_model_all = sklearn.ensemble.RandomForestRegressor()\n",
      "    rf_model_all.fit(X,y)\n",
      "    \n",
      "    # Generate predictions on all data.\n",
      "    y_hat_all = rf_model_all.predict(X)\n",
      "    \n",
      "    # Calculate performance metrics.\n",
      "    performance_all_r2 = sklearn.metrics.r2_score(y, y_hat_all)\n",
      "    performance_all_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y, y_hat_all))\n",
      "    performance_all_sample_mean = y.mean()\n",
      "    \n",
      "    # Output performance metrics.\n",
      "    if print_metrics:\n",
      "        print 'Model Built on All Data vs. Performance on All Data:'\n",
      "        print '\\tR^2 on all data:  %.4f' % performance_all_r2\n",
      "        print '\\tRMSE on all data: %.4f' % performance_all_rmse\n",
      "        print '\\tAll data sample mean (for reference): %.4f' % performance_all_sample_mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model Generation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Regression Models"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Park Street"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_prediction(1052, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.9231\n",
        "\tRMSE on test data: 1081.2493\n",
        "\tTest sample mean (for reference): 11435.6795\n",
        "Model Built on All Data vs. Performance on All Data:\n",
        "\tR^2 on all data:  0.9841\n",
        "\tRMSE on all data: 518.2252\n",
        "\tAll data sample mean (for reference): 10813.8175\n"
       ]
      }
     ],
     "prompt_number": 609
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Downtown Crossing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_prediction(1039, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.9887\n",
        "\tRMSE on test data: 517.9478\n",
        "\tTest sample mean (for reference): 10757.2564\n",
        "Model Built on All Data vs. Performance on All Data:\n",
        "\tR^2 on all data:  0.9965\n",
        "\tRMSE on all data: 280.5236\n",
        "\tAll data sample mean (for reference): 11247.1054\n"
       ]
      }
     ],
     "prompt_number": 610
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Harvard"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_prediction(1035, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.8911\n",
        "\tRMSE on test data: 872.2537\n",
        "\tTest sample mean (for reference): 8405.0385\n",
        "Model Built on All Data vs. Performance on All Data:\n",
        "\tR^2 on all data:  0.9724\n",
        "\tRMSE on all data: 389.9957\n",
        "\tAll data sample mean (for reference): 8872.7301\n"
       ]
      }
     ],
     "prompt_number": 611
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Tufts Medical Center"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_prediction(1079, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.9831\n",
        "\tRMSE on test data: 164.8300\n",
        "\tTest sample mean (for reference): 2604.7564\n",
        "Model Built on All Data vs. Performance on All Data:\n",
        "\tR^2 on all data:  0.9939\n",
        "\tRMSE on all data: 95.8725\n",
        "\tAll data sample mean (for reference): 2678.7912\n"
       ]
      }
     ],
     "prompt_number": 612
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Park Street"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_prediction(1052, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.9317\n",
        "\tRMSE on test data: 1135.0049\n",
        "\tTest sample mean (for reference): 10492.6410\n",
        "Model Built on All Data vs. Performance on All Data:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tR^2 on all data:  0.9905\n",
        "\tRMSE on all data: 400.5256\n",
        "\tAll data sample mean (for reference): 10813.8175\n"
       ]
      }
     ],
     "prompt_number": 613
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Harvard"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_prediction(1035, print_metrics=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model Built on Training Data vs. Performance on Test Data:\n",
        "\tR^2 on test data:  0.9347\n",
        "\tRMSE on test data: 602.0013\n",
        "\tTest sample mean (for reference): 8650.8462\n",
        "Model Built on All Data vs. Performance on All Data:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tR^2 on all data:  0.9778\n",
        "\tRMSE on all data: 349.5539\n",
        "\tAll data sample mean (for reference): 8872.7301\n"
       ]
      }
     ],
     "prompt_number": 614
    }
   ],
   "metadata": {}
  }
 ]
}