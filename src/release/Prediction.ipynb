{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions for the MBTA #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation ##\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries.\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MBTA ####\n",
    "\n",
    "The MBTA provided us with entry data for each station at 15 minute intervals. After some wrangling, our base data set was composed of a ***stations*** data set and a ***gate count*** data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stations #####\n",
    "\n",
    "The station data set contained basic information for each station. One row corresponded to one station. Some of the data, in particular the latitude and longitude, was scraped from the web.\n",
    "\n",
    "- ***stationid***: The unique identifier for the station.\n",
    "- ***name***: The full name of the station station.\n",
    "- ***line_1***: The primary line of the station (red/green/blue/orange).\n",
    "- ***line_2***: The secondary line of the station (red/green/blue/orange). Only a few stations, such as park street (green/red line), had a value for this field.\n",
    "- ***lat***: The latitude of the station.\n",
    "- ***lon***: The longitude of the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = pd.read_csv('../../../data/stations.csv', low_memory=False)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gate Count #####\n",
    "\n",
    "The gate count data set contained basic information for entries at each station. One row corresponded to the number of entries at a particular 15 minute interval for a station.\n",
    "\n",
    "- ***locationid***: The unique identifier for the station.\n",
    "- ***entries***: The number of entries for 15 minute interval.\n",
    "- ***exits***: The number of exits for the 15 minute interval (NOT USED - exists are unreliable due to the nature of the system).\n",
    "- ***service_day***: The actual day the service started (services on weekends can run into the next day).\n",
    "- ***service_datetime***: The 15 minute interval where the entries/exists were aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatecounts = pd.read_csv('../../../data/gatecounts.csv', low_memory=False)\n",
    "gatecounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather ####\n",
    "\n",
    "The weather data was obtained by scraping the [wunderground](http://www.wunderground.com/) API. Each row corresponded to the weather for a single day in Boston. A sample of the data is provided below. The most important feature was the \"snow_fall\" column, which was the recorded snow fall for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../../../data/weather.csv', low_memory=False)\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game Days ####\n",
    "\n",
    "The game day data for Red Sox games was scraped directly from their website while the game day data for the Celtics and Bruines game was scraped from ESPN. Each row corresponded to a day in which a game (or games) occurred. A sample of the data is provided below. The most important features for daily prediction purposes were:\n",
    "\n",
    "- ***bruins_game***: Binary column representing if a Bruins game occurred on this day.\n",
    "- ***celtics_game***: Binary column representing if a Celtics game occurred on this day.\n",
    "- ***sox_game***: Binary column representing if a Red Sox game occurred on this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamedays = pd.read_csv('../../../data/game_days.csv', low_memory=False)\n",
    "gamedays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Data ####\n",
    "\n",
    "The daily data set was an aggregation of entries per day for each station, with a few additional features. Sparing the details on how we generated these additional features (please refer to the ***features*** section of our repository), some important ones to recognize are:\n",
    "\n",
    "- ***entries_weeks_ago_1***: The number of entries for this station on the same day one week ago.\n",
    "- ***snow_fall***: The amount of snow the fell for that day, in inches.\n",
    "- ***snow_accum***: The amount of snow accumulated up to the current day. The snow accumulation was calculated using a quasi-linear decay function based on the snow fall of the previous two weeks.\n",
    "- ***dist_to_center***: The distance (in kilometers) to the center of the city (city hall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Data w/o Holidays ####\n",
    "\n",
    "After some thorough analysis, we decided that \"holidays\" had no consistent pattern. We decided to consider them \"extreme outliers\" and removed them from our base data set (future analysis would be necessary for holidays). The \"holidays\" we removed were:\n",
    "\n",
    "- New Years Eve\n",
    "- New Years Day\n",
    "- Boston Marathon\n",
    "- Fourth of July\n",
    "- Boston Move Out Day (September 1st)\n",
    "- Thanksgiving\n",
    "- Black Friday\n",
    "- Christmas Week (12/25, 12/26, 12/27)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Data w/o Weekends ####\n",
    "\n",
    "After some thorough analysis, we decided that weekends and weekdays had two completely seperate patterns. We wanted to assist the MBTA in the greatest way possible and concentrated on weekdays, removing weekends from our base data set (future analysis would be necessary for weekends)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Data ####\n",
    "\n",
    "Our final data set consisted of daily entries for each station, exluding holidays and weekends, with the additional weather and game features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 32547\n",
      "Cols: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locationid</th>\n",
       "      <th>service_day</th>\n",
       "      <th>entries</th>\n",
       "      <th>name</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>service_datetime</th>\n",
       "      <th>fog</th>\n",
       "      <th>...</th>\n",
       "      <th>snow_fall_predict</th>\n",
       "      <th>snow_accum</th>\n",
       "      <th>snow_accum_predict</th>\n",
       "      <th>dist_to_center</th>\n",
       "      <th>entries_weeks_ago_1</th>\n",
       "      <th>entries_weeks_ago_2</th>\n",
       "      <th>entries_weeks_ago_3</th>\n",
       "      <th>bruins_game</th>\n",
       "      <th>celtics_game</th>\n",
       "      <th>sox_game</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1002</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 5733</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.329550</td>\n",
       "      <td>-71.056960</td>\n",
       "      <td> 2013-01-03 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>  3.404767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1004</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 5842</td>\n",
       "      <td>    JFK/U Mass</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.321438</td>\n",
       "      <td>-71.052393</td>\n",
       "      <td> 2013-01-03 04:45:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>  4.328881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1005</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 6139</td>\n",
       "      <td>  North Quincy</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.274816</td>\n",
       "      <td>-71.029176</td>\n",
       "      <td> 2013-01-03 03:15:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>  9.777437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1006</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 4028</td>\n",
       "      <td>     Wollaston</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.265615</td>\n",
       "      <td>-71.019402</td>\n",
       "      <td> 2013-01-03 05:00:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 10.976943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1007</td>\n",
       "      <td> 2013-01-03 00:00:00</td>\n",
       "      <td> 7331</td>\n",
       "      <td> Quincy Center</td>\n",
       "      <td> Red</td>\n",
       "      <td> NaN</td>\n",
       "      <td> 42.250879</td>\n",
       "      <td>-71.004798</td>\n",
       "      <td> 2013-01-03 04:30:00</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 12.909591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   locationid          service_day  entries           name line_1 line_2  \\\n",
       "0        1002  2013-01-03 00:00:00     5733  Andrew Square    Red    NaN   \n",
       "1        1004  2013-01-03 00:00:00     5842     JFK/U Mass    Red    NaN   \n",
       "2        1005  2013-01-03 00:00:00     6139   North Quincy    Red    NaN   \n",
       "3        1006  2013-01-03 00:00:00     4028      Wollaston    Red    NaN   \n",
       "4        1007  2013-01-03 00:00:00     7331  Quincy Center    Red    NaN   \n",
       "\n",
       "         lat        lon     service_datetime  fog ...   snow_fall_predict  \\\n",
       "0  42.329550 -71.056960  2013-01-03 05:00:00    0 ...                   0   \n",
       "1  42.321438 -71.052393  2013-01-03 04:45:00    0 ...                   0   \n",
       "2  42.274816 -71.029176  2013-01-03 03:15:00    0 ...                   0   \n",
       "3  42.265615 -71.019402  2013-01-03 05:00:00    0 ...                   0   \n",
       "4  42.250879 -71.004798  2013-01-03 04:30:00    0 ...                   0   \n",
       "\n",
       "   snow_accum  snow_accum_predict  dist_to_center  entries_weeks_ago_1  \\\n",
       "0           0                   0        3.404767                  NaN   \n",
       "1           0                   0        4.328881                  NaN   \n",
       "2           0                   0        9.777437                  NaN   \n",
       "3           0                   0       10.976943                  NaN   \n",
       "4           0                   0       12.909591                  NaN   \n",
       "\n",
       "   entries_weeks_ago_2  entries_weeks_ago_3  bruins_game  celtics_game  \\\n",
       "0                  NaN                  NaN            0             0   \n",
       "1                  NaN                  NaN            0             0   \n",
       "2                  NaN                  NaN            0             0   \n",
       "3                  NaN                  NaN            0             0   \n",
       "4                  NaN                  NaN            0             0   \n",
       "\n",
       "   sox_game  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../../data/mbta_daily_weather_games_noholidays_weekdays.csv', low_memory=False)\n",
    "print(\"Rows: \" + str(data.shape[0]))\n",
    "print(\"Cols: \" + str(data.shape[1]))\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prediction ##\n",
    "\n",
    "### Introduction ###\n",
    "\n",
    "TODO: FIX\n",
    "\n",
    "Changes in the accuracy of entry predictions are one way of determining the importance of snow to the MBTA. To do so, we can add snow \"features\" (or predictors) to a basic model (Linear Regression) and check for a reduction in the overall percent error of our predictions for each station. We can define percent error for each station as:\n",
    "\n",
    "$$ station_{percent\\ error} = \\frac{|entries_{predicted} - entries_{actual}|}{entries_{actual}} * 100 $$\n",
    "\n",
    "Once we have determined the percent error for each station, we can determine the overall average of errors:\n",
    "\n",
    "$$ \\ Station \\ Error = \\frac{1}{n} \\sum^n_i{(\\frac{|entries_{predicted} - entries_{actual}|}{entries_{actual}})} $$\n",
    "\n",
    "$$ \\ Station \\ Error = \\frac{\\sum^n_i{|entries_{predicted} - entries_{actual}|}}{\\sum^n_i{entries_{actual}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Definitions ###\n",
    "\n",
    "The following methods are used to help us generate a prediction model and determine the percent error for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates a prediction model for a given station with desired features.\n",
    "\n",
    "This will return the best model based on 10 fold cross-validation with an 70/30 train/test split.\n",
    "\n",
    "In addition, this will return the mean absolute error for all the entries in the test set, and any outliers\n",
    "based on a given absolute error threshold.\n",
    "\n",
    "Key arguments:\n",
    "\n",
    "  station    -- The station to train on.\n",
    "  cols       -- The columns in the station data frame that are considered features.\n",
    "  predictor  -- The predictor to use (sklearn predictor)\n",
    "  parameters -- The parameters for the predictor.\n",
    "  rstate     -- The random state to use for the train/test split.\n",
    "\n",
    "\"\"\"\n",
    "def predict(station, features, predictor, parameters, rstate):\n",
    "    # Copy the station so we don't manipulate the original.\n",
    "    station = station.copy()\n",
    "    \n",
    "    # Get the columns of the dataframe.\n",
    "    all_cols = list(station.columns)\n",
    "    \n",
    "    # Determine the indices of the columns.\n",
    "    y_col_indices = [0] + list(np.where([col == 'entries' for col in all_cols])[0] + 1)\n",
    "    x_col_indices = [0] + list(np.where([col in features for col in all_cols])[0] + 1)\n",
    "    \n",
    "    # Make sure none of the predictor fields are null.\n",
    "    for col in features:\n",
    "        station = station[pd.notnull(station[col])]\n",
    "    \n",
    "    # Remove any entries where no one was there (probably closed) or the entries appears to be\n",
    "    #  extremely low (less than 1% of the mean).\n",
    "    # We assume that any entries with less than 1% of the normal entries is having serious issues.\n",
    "    min_entries = np.mean(station['entries']) * .005\n",
    "    station = station[station['entries'] > min_entries]\n",
    "    \n",
    "    # Reset the station indices, we have to reset twice so the matrix values gets the index column.\n",
    "    station = station.reset_index()\n",
    "    station.drop('index', axis=1, inplace=True)\n",
    "    station = station.reset_index()\n",
    "    \n",
    "    # Get the dataframe as a matrix where the first column is the index.\n",
    "    matrix = station.values\n",
    "    \n",
    "    # Slice so the y only contains 2 column (index, entries)\n",
    "    #  and the x is a matrix that contains the index and all the predictors.\n",
    "    y = matrix[:,y_col_indices]\n",
    "    x = matrix[:,x_col_indices]\n",
    "    \n",
    "    # Split the data set into a train and test.\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.3, random_state=rstate)\n",
    "    \n",
    "    # Convert the train and test sets into a format sklean fit() expects.\n",
    "    x_train_fit = np.matrix(x_train[:,1:], dtype=np.float32)\n",
    "    y_train_fit = np.array([v[0] for v in y_train[:,1:]], dtype=np.uint16)\n",
    "    \n",
    "    x_test_fit = np.matrix(x_test[:,1:], dtype=np.float32)\n",
    "    y_test_fit = np.array([v[0] for v in y_test[:,1:]], dtype=np.uint16)\n",
    "    \n",
    "    # Train using a grid search based on the parameters provided.\n",
    "    clf = grid_search.GridSearchCV(predictor, parameters, scoring='mean_squared_error', cv=10)\n",
    "    clf.fit(x_train_fit, y_train_fit)\n",
    "    \n",
    "    # Determine what the best model was.\n",
    "    model = clf.best_estimator_\n",
    "    \n",
    "    # Find the train/test data sets in the original station data frame.\n",
    "    train_results = station.iloc[y_train[:,0]]\n",
    "    test_results = station.iloc[y_test[:,0]]\n",
    "    \n",
    "    # Predict using the train/test set.\n",
    "    train_results['prediction'] = model.predict(x_train_fit)\n",
    "    test_results['prediction'] = model.predict(x_test_fit)\n",
    "    \n",
    "    # Determine the percent errors.\n",
    "    train_results['error'] = np.abs(train_results['prediction'] - train_results['entries']) / train_results['entries']\n",
    "    test_results['error'] = np.abs(test_results['prediction'] - test_results['entries']) / test_results['entries']\n",
    "    \n",
    "    # Determine overall error for station for training and test.\n",
    "    train_error =  np.sum(np.abs(train_results['prediction'] - train_results['entries'])) / np.sum(train_results['entries'])\n",
    "    test_error =  np.sum(np.abs(test_results['prediction'] - test_results['entries'])) / np.sum(test_results['entries'])\n",
    "    \n",
    "    return model, train_error, test_error, train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests a station multiple times by running the prediction method with a different\n",
    "random state.\n",
    "\n",
    "Key arguments:\n",
    "\n",
    "  station    -- The station to test for.\n",
    "  cols       -- The columns in station to use as features.\n",
    "  predictor  -- The predictor to use (sklearn predictor)\n",
    "  parameters -- The parameters for the predictor.\n",
    "  trials     -- The number of times to run predict (also the number of randomly generated test sets).\n",
    "\n",
    "\"\"\"\n",
    "def test_station(station, cols, predictor, parameters, error_threshold = 0.2, trials = 50):\n",
    "    train_errors = np.zeros(trials)\n",
    "    test_errors = np.zeros(trials)\n",
    "    train_outliers = pd.DataFrame()\n",
    "    test_outliers = pd.DataFrame()\n",
    "    \n",
    "    for rstate in xrange(trials):\n",
    "        model, train_error, test_error, train_results, test_results = predict(station, cols, predictor, parameters, rstate)\n",
    "        \n",
    "        # Keep track of the train and test errors for this result.\n",
    "        train_errors[rstate] = train_error\n",
    "        test_errors[rstate] = test_error\n",
    "        \n",
    "        # Keep track of any \"outliers\" for both the train and test group, i.e. rows that were above the threshold.\n",
    "        trial_train_outliers = train_results[train_results['error'] > error_threshold]\n",
    "        trial_test_outliers = test_results[test_results['error'] > error_threshold]\n",
    "        \n",
    "        if len(train_outliers) == 0 and len(test_outliers) == 0:\n",
    "            train_outliers = trial_train_outliers\n",
    "            test_outliers = trial_test_outliers\n",
    "        else:\n",
    "            train_outliers = pd.concat([train_outliers, trial_train_outliers[~trial_train_outliers.isin(train_outliers.index)]])\n",
    "            test_outliers = pd.concat([test_outliers, trial_test_outliers[~trial_test_outliers.isin(test_outliers.index)]])\n",
    "    \n",
    "    return train_errors, test_errors, train_outliers, test_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example ###\n",
    "\n",
    "Lets step through a simple example of how the process works. For this simple example, we're going to predict the entries for Andrew Square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "andrew_square = data[data['locationid'] == 1002]\n",
    "predictor = linear_model.LinearRegression()\n",
    "parameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a group of days (test set), we want to see how accruate we can predict the number of daily entries for Andrew Square using only the number of entries on the same day for the past two weeks.\n",
    "\n",
    "E.g. If I was predicting the entries for Andrew Square on 2015-02-21, I would build a regression model only using the known entries on 2015-02-14 and 2015-02-07 as features.\n",
    "\n",
    "The method below builds a model and predicts the entries for multiple test sets. We can than average the error for each test set to determine the mean prediction error for Andrew Square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_errors, test_errors, train_outliers, test_outliers = test_features(andrew_square, ['entries_weeks_ago_1', 'entries_weeks_ago_2'], predictor, parameters, .2, 50)\n",
    "print(\"Average Percent Error for Test Sets: \" + str(np.mean(test_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _We can predict entries at Andrew Square with an 6% error rate._\n",
    "\n",
    "We can also take a look at days in which the prediction model did not perform well (> 20% error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Feature Set ###\n",
    "\n",
    "Determining features is generally the first step of the model generation process. For our scenario, we wanted to determine the feature set that results in the lowest possible average percent error across all stations. To accomplish this, we executed an exhaustive search of feature sets to determine which performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests all stations with a set of features.\n",
    "\n",
    "Key arguments:\n",
    "\n",
    "  features -- The features to test.\n",
    "\n",
    "\"\"\"\n",
    "def test_all_stations(predictor, parameters, features):\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Test the feature sets for every single station.\n",
    "    for locationid in np.unique(data['locationid']):\n",
    "        result = pd.Series()\n",
    "        \n",
    "        # Get a particular station.\n",
    "        station = data[data['locationid'] == locationid]\n",
    "        \n",
    "        # Add identifier information.\n",
    "        result['locationid'] = locationid\n",
    "        result['name'] = station['name'].iloc[0]\n",
    "        \n",
    "        # Get the mean percent error for the train/test set using the base feature.\n",
    "        train_errors, test_errors, _, _ = test_station(station, features, predictor, parameters)\n",
    "        result['train_error'] = np.mean(train_errors)\n",
    "        result['test_error'] = np.mean(test_errors)\n",
    "        result['features'] = str(features)\n",
    "        \n",
    "        # Save the result.\n",
    "        results = results.append(result, ignore_index=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Feature Set ####\n",
    "\n",
    "This feature set was included in all feature combinations (consider it the baseline feature set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_features = ['entries_weeks_ago_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Feature Set ####\n",
    "\n",
    "The \"additional\" features we wanted to test included weather, sports, and other aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "other_features = [\n",
    "    'entries_weeks_ago_2', \n",
    "    'entries_weeks_ago_3',\n",
    "    ['day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4'],\n",
    "    ['month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12'],\n",
    "    ['temp_min', 'temp_max', 'temp_mean'],\n",
    "    ['rain', 'rain_fall'],\n",
    "    ['snow', 'snow_fall', 'snow_accum'],\n",
    "    ['bruins_game', 'celtics_game', 'sox_game'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhaustive Search ####\n",
    "\n",
    "For all possible combinations of \"other features\", we determined the average percent error across all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "# Use linear regression since it is simple and fast.\n",
    "predictor = linear_model.LinearRegression()\n",
    "parameters = {}\n",
    "\n",
    "i = 0\n",
    "# Try every combination of the \"other features\".\n",
    "for l in range(0, len(other_features) + 1):\n",
    "    for subset in itertools.combinations(other_features, l):\n",
    "        # Add on the base line features.\n",
    "        features = list(base_features)\n",
    "        # Build the feature list.\n",
    "        for feature in subset:\n",
    "            features += [feature] if not isinstance(feature, list) else feature\n",
    "        \n",
    "        # Save results to disk, since this takes a while and we don't want to lose results.\n",
    "        results = results.append(test_features_for_all_stations(predictor, parameters, features), ignore_index=True)\n",
    "        results.to_csv(\"../../results/prediction/feature_set_\" + str(i) + \".csv\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureset_results = pd.read_csv(\"../../results/prediction/feature_set_eval.csv\", low_memory=False)\n",
    "featureset_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = featureset_results.groupby('features').mean().sort('test_error')\n",
    "grouped.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_features2 = ['entries_weeks_ago_1', 'entries_weeks_ago_2', 'entries_weeks_ago_3', 'day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'snow', 'snow_fall', 'snow_accum', 'bruins_game', 'celtics_game', 'sox_game']\n",
    "top_group2 = featureset_results[featureset_results.features == str(top_features2)]\n",
    "print(top_group2['test_error'].mean())\n",
    "print(top_group2['test_error'].median())\n",
    "print(top_group2['train_error'].mean())\n",
    "print(top_group2['train_error'].median())\n",
    "top_group2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_group2[top_group2.test_error > .1].head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion #####\n",
    "\n",
    "We found the best feature set to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Model ###\n",
    "\n",
    "The next step of the process is to determine the best algorithm (model) for prediction. We decided to test the following algorithms using the best features we determined before.\n",
    "\n",
    "- Ridge Regression\n",
    "- Random Forrest\n",
    "- SVM (Support Vector Machines are often the de facto default algorithm to use for a lot of machine learning scenarios. However, based on some preliminary results we decided not to continue testing with SVMs. The time to train was not worth the trivial gain in accuracy - it was taking hours to train a single station!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = ['entries_weeks_ago_1', 'entries_weeks_ago_2', 'entries_weeks_ago_3', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'snow', 'snow_fall', 'snow_accum', 'bruins_game', 'celtics_game', 'sox_game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests a station multiple times by running the prediction method with a different\n",
    "random state and records the best parameters for each run.\n",
    "\n",
    "Key arguments:\n",
    "\n",
    "  station       -- The station to test for.\n",
    "  predictor     -- The predictor to use (sklearn predictor)\n",
    "  parameters    -- The parameters for the predictor.\n",
    "  param_results -- The object to store the parameter results in.\n",
    "  trials        -- The number of times to run predict (also the number of randomly generated test sets).\n",
    "\n",
    "\"\"\"\n",
    "def test_params(station, predictor, parameters, param_results, trials = 50):\n",
    "    for rstate in xrange(trials):\n",
    "        model, train_error, test_error, train_results, test_results = predict(station, best_features, predictor, parameters, rstate)\n",
    "        \n",
    "        param_results = param_results.append({'train_error': train_error, 'test_errors': test_error, 'params': str(model.get_params())}, ignore_index=True)\n",
    "    \n",
    "    return param_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression ####\n",
    "\n",
    "Ridge regression requires a magic number (alpha) that controls the amount of shrinkage (robustness to co-linear features). One approach to finding the best alpha for our environment is to run a grid search with a wide range of alphas on every station. Once the best alpha is determined, it will used as the default alpha for the final ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_results = pd.DataFrame()\n",
    "\n",
    "for locationid in np.unique(data['locationid']):\n",
    "    station = data[data['locationid'] == locationid]\n",
    "    \n",
    "    predictor = linear_model.Ridge()\n",
    "    parameters = {'alpha': range(0, 101)}\n",
    "    \n",
    "    param_results = test_params(station, predictor, parameters, param_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = param_results.groupby('params').agg(['mean', 'count'])\n",
    "grouped['test_errors'].sort('count', ascending=False).to_csv(\"ridge-param-results.csv\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor = linear_model.Ridge()\n",
    "parameters = {'alpha': [1.0]}\n",
    "ridge_regression_results = test_all_stations(predictor, parameters, best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0761073602689\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>locationid</th>\n",
       "      <th>name</th>\n",
       "      <th>test_error</th>\n",
       "      <th>train_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> ['entries_weeks_ago_1', 'entries_weeks_ago_2',...</td>\n",
       "      <td> 1002</td>\n",
       "      <td> Andrew Square</td>\n",
       "      <td> 0.051160</td>\n",
       "      <td> 0.047298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> ['entries_weeks_ago_1', 'entries_weeks_ago_2',...</td>\n",
       "      <td> 1004</td>\n",
       "      <td>    JFK/U Mass</td>\n",
       "      <td> 0.098075</td>\n",
       "      <td> 0.092778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> ['entries_weeks_ago_1', 'entries_weeks_ago_2',...</td>\n",
       "      <td> 1005</td>\n",
       "      <td>  North Quincy</td>\n",
       "      <td> 0.058974</td>\n",
       "      <td> 0.053658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> ['entries_weeks_ago_1', 'entries_weeks_ago_2',...</td>\n",
       "      <td> 1006</td>\n",
       "      <td>     Wollaston</td>\n",
       "      <td> 0.050891</td>\n",
       "      <td> 0.046671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> ['entries_weeks_ago_1', 'entries_weeks_ago_2',...</td>\n",
       "      <td> 1007</td>\n",
       "      <td> Quincy Center</td>\n",
       "      <td> 0.058022</td>\n",
       "      <td> 0.054156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  locationid  \\\n",
       "0  ['entries_weeks_ago_1', 'entries_weeks_ago_2',...        1002   \n",
       "1  ['entries_weeks_ago_1', 'entries_weeks_ago_2',...        1004   \n",
       "2  ['entries_weeks_ago_1', 'entries_weeks_ago_2',...        1005   \n",
       "3  ['entries_weeks_ago_1', 'entries_weeks_ago_2',...        1006   \n",
       "4  ['entries_weeks_ago_1', 'entries_weeks_ago_2',...        1007   \n",
       "\n",
       "            name  test_error  train_error  \n",
       "0  Andrew Square    0.051160     0.047298  \n",
       "1     JFK/U Mass    0.098075     0.092778  \n",
       "2   North Quincy    0.058974     0.053658  \n",
       "3      Wollaston    0.050891     0.046671  \n",
       "4  Quincy Center    0.058022     0.054156  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ridge_regression_results['test_error'].mean())\n",
    "ridge_regression_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forrest #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_results = pd.DataFrame()\n",
    "\n",
    "for locationid in np.unique(data['locationid']):\n",
    "    station = data[data['locationid'] == locationid]\n",
    "    \n",
    "    predictor = ensemble.RandomForestRegressor()\n",
    "    parameters = {'n_estimators': range(10, 20)}\n",
    "    \n",
    "    param_results = test_params(station, predictor, parameters, param_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 19, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}</th>\n",
       "      <td> 0.075936</td>\n",
       "      <td> 446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 18, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}</th>\n",
       "      <td> 0.075987</td>\n",
       "      <td> 427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 17, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}</th>\n",
       "      <td> 0.077465</td>\n",
       "      <td> 395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 16, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}</th>\n",
       "      <td> 0.077740</td>\n",
       "      <td> 375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 15, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}</th>\n",
       "      <td> 0.074958</td>\n",
       "      <td> 350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          mean  \\\n",
       "params                                                                                                                                                                                                                                                                                                           \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 19, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}  0.075936   \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 18, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}  0.075987   \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 17, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}  0.077465   \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 16, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}  0.077740   \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 15, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}  0.074958   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      count  \n",
       "params                                                                                                                                                                                                                                                                                                       \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 19, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}    446  \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 18, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}    427  \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 17, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}    395  \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 16, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}    375  \n",
       "{'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'min_density': None, 'compute_importances': None, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 15, 'min_samples_split': 2, 'random_state': None, 'criterion': 'mse', 'max_features': 'auto', 'max_depth': None}    350  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = param_results.groupby('params').agg(['mean', 'count'])\n",
    "grouped['test_errors'].sort('count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = param_results.groupby('params').agg(['mean', 'count'])\n",
    "grouped['test_errors'].sort('count', ascending=False).to_csv(\"randomforrest-param-results.csv\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "- *What are the most important features?*\n",
    "\n",
    "- *What was the best model?*\n",
    "\n",
    "- *What is the average prediction ratio?*\n",
    "\n",
    "- *What are some known outliers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
